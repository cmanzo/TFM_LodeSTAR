{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt \n",
    "from numpy.random import uniform\n",
    "import numpy as np\n",
    "\n",
    "image_size = 32\n",
    "\n",
    "# particle1 = dt.PointParticle(\n",
    "#             intensity = 2.0,\n",
    "#             # radius=(10, 10)*dt.units.pixel,\n",
    "#             position=lambda: uniform(image_size / 2 - 15, image_size / 2 + 15, size=2),\n",
    "# )\n",
    "\n",
    "particle1 = dt.Ellipse(\n",
    "            intensity = 5.0,\n",
    "            position_unit=\"pixel\",\n",
    "            position=lambda: uniform(image_size / 2 - 5, image_size / 2 + 5, size=2),\n",
    "            radius=(5, 5)*dt.units.pixel,\n",
    "        )\n",
    "\n",
    "particle2 = dt.Ellipse(\n",
    "            intensity = 5.0,\n",
    "            position_unit=\"pixel\",\n",
    "            position=lambda: uniform(image_size / 2 - 5, image_size / 2 + 5, size=2),\n",
    "            radius=(5, 12)*dt.units.pixel,\n",
    "            rotation=lambda: np.random.uniform(0, 2 * np.pi),   \n",
    "        )\n",
    "\n",
    "optics = dt.Fluorescence(output_region=(0, 0, image_size, image_size))\n",
    "\n",
    "import torch\n",
    "\n",
    "simulation1 = (\n",
    "    optics(particle1) \n",
    "    >> dt.NormalizeMinMax(0, 1)\n",
    "    >> dt.Gaussian(sigma=0.1)\n",
    "    >> dt.MoveAxis(-1, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
    ")\n",
    "\n",
    "simulation2 = (\n",
    "    optics(particle2) \n",
    "    >> dt.NormalizeMinMax(0, 1)\n",
    "    >> dt.Gaussian(sigma=0.1)\n",
    "    >> dt.MoveAxis(-1, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_dataset1 = dt.pytorch.Dataset(simulation1 & [0], length=100)\n",
    "train_dataset2 = dt.pytorch.Dataset(simulation2 & [1], length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "from matplotlib import pyplot as plt\n",
    "train_dataset = ConcatDataset([train_dataset1, train_dataset2])\n",
    "# train_dataset = train_dataset1\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(5):\n",
    "\t\n",
    "\timage, lab  = train_dataset[int(i)]\n",
    "\tplt.subplot(1, 5, i+1)\n",
    "\tplt.imshow(image[0], cmap=\"gray\", origin=\"lower\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(5):\n",
    "\t\n",
    "\timage, lab  = train_dataset[int(i)+100]\n",
    "\tplt.subplot(1, 5, i+1)\n",
    "\tplt.imshow(image[0], cmap=\"gray\", origin=\"lower\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import morphology\n",
    "\n",
    "from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "from deeplay.applications.detection.lodestar.transforms import (\n",
    "    RandomRotation2d,\n",
    "    RandomTranslation2d,\n",
    "    Transforms,\n",
    ")\n",
    "\n",
    "\n",
    "class LodeSTAR(Application):\n",
    "\n",
    "    # num_outputs: int # only 2D for now\n",
    "    num_classes: int\n",
    "    transforms: Transforms\n",
    "    n_transforms: int\n",
    "    model: nn.Module\n",
    "    between_loss: Callable\n",
    "    within_loss: Callable\n",
    "    between_loss_weight: float\n",
    "    within_loss_weight: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[nn.Module] = None,\n",
    "        # num_outputs: int = 2,\n",
    "        num_classes: int = 2,\n",
    "        transforms: Optional[Transforms] = None,\n",
    "        n_transforms: int = 2,\n",
    "        between_loss: Optional[Callable] = None,\n",
    "        within_loss: Optional[Callable] = None,\n",
    "        between_loss_weight: float = 1,\n",
    "        within_loss_weight: float = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if transforms is None:\n",
    "            transforms = Transforms(\n",
    "                [\n",
    "                    RandomTranslation2d(),\n",
    "                    RandomRotation2d(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.n_transforms = n_transforms\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.between_loss_weight = between_loss_weight\n",
    "        self.within_loss_weight = within_loss_weight\n",
    "\n",
    "        super().__init__(loss=None, **kwargs)\n",
    "\n",
    "    def _get_default_model(self):\n",
    "        cnn = ConvolutionalNeuralNetwork(\n",
    "            None,\n",
    "            [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "            (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "        )\n",
    "        cnn.blocks[2].pooled()\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def transform_data(self, batch):\n",
    "        repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "        transformed, inverse = self.transforms(repeated)\n",
    "        return transformed, inverse\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            x, class_label = x\n",
    "\n",
    "        else:\n",
    "            x, _, _ = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        y = out[:, :3, ...]\n",
    "        classes=out[:, 3:, ...]\n",
    "        classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "        batch_size = classes.size(0)\n",
    "        num_channels = classes.size(1)\n",
    "        _, _, Hx, Wx = x.shape\n",
    "        _, _, Hy, Wy = y.shape\n",
    "        x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "        y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        if self.training:\n",
    "            x_range = x_range - Hx / 2 + 0.5\n",
    "            y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "            batch_indices = torch.arange(batch_size)\n",
    "            \n",
    "            # mk = torch.ones((batch_size, num_channels), dtype=torch.bool)\n",
    "            # mk[batch_indices, class_label.squeeze()] = False\n",
    "            # mk[:, -1] = False  # Set the last channel to False for all batches\n",
    "\n",
    "            # mk = mk.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, Hy, Wy)\n",
    "            # classes[mk] *= 0.0\n",
    "            # classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "            mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "        else:\n",
    "            \n",
    "            mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "        Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "        delta_x = y[:, 0:1, ...]\n",
    "        delta_y = y[:, 1:2, ...]\n",
    "        yy = y[:, 2:3, ...]\n",
    "        weights = y[:, 2:3, ...].sigmoid()\n",
    "        X = X + delta_x\n",
    "        Y = Y + delta_y\n",
    "        \n",
    "        return torch.cat(\n",
    "            [X, Y, weights, mask, classes], dim=1\n",
    "        )\n",
    "\n",
    "    def normalize(self, weights):\n",
    "        weights = weights + 1e-5\n",
    "        return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    def reduce(self, X, weights):\n",
    "        return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "    def compute_loss(self, y_hat, inverse_fn):\n",
    "        B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "        y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:] # Changed\n",
    "\n",
    "        weights =  mask_gumbel #\n",
    "        weights = self.normalize(weights)\n",
    "        y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "        within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "        within_disagreement_loss = self.within_loss(\n",
    "            within_disagreement, torch.zeros_like(within_disagreement)\n",
    "        )\n",
    "\n",
    "        y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "        between_disagreement_loss = 0\n",
    "\n",
    "        for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "            batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "            batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "                batch_preds\n",
    "            )\n",
    "            between_disagreement_loss += (\n",
    "                self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "            )\n",
    "        weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "        weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "        compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "        mask_loss = 10*compl_mask.mean(dim=(2, 3)).mean()# #- mask_gumbel.mean()\n",
    "\n",
    "        return {\n",
    "            \"between_image_disagreement\": weighted_between_loss,\n",
    "            \"within_image_disagreement\": weighted_within_loss,\n",
    "            \"mask_loss\": mask_loss,\n",
    "        }\n",
    "\n",
    "    # def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "    #     \"\"\"Detects objects in a batch of images\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     x : array-like\n",
    "    #         Input to model\n",
    "    #     alpha, beta: float\n",
    "    #         Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "    #     cutoff: float\n",
    "    #         Threshold for detection\n",
    "    #     mode: string\n",
    "    #         Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "    #         `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "    #         score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "    #     \"\"\"\n",
    "    #     y, classes = self(x.to(self.device))\n",
    "    #     y_pred, weights, mask_gumbel = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:, ...] # Changed\n",
    "    #     detections = [\n",
    "    #         self.detect_single(y_pred[i], weights[i], alpha, beta, cutoff, mode)\n",
    "    #         for i in range(len(y_pred))\n",
    "    #     ]\n",
    "\n",
    "    #     return detections\n",
    "\n",
    "    # def pooled(self, x, mask=1):\n",
    "    #     \"\"\"Pooled output from model.\n",
    "\n",
    "    #     Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "    #     Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     x : array-like\n",
    "    #         Input to model\n",
    "    #     mask : array-like\n",
    "    #         Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "    #     \"\"\"\n",
    "    #     y, classes = self(x.to(self.device))\n",
    "    #     y_pred, weights, mask_gumbel = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:, ...] # Changed\n",
    "    #     masked_weights = weights * mask\n",
    "\n",
    "    #     pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "    #     return pooled\n",
    "\n",
    "    # def detect_single(\n",
    "    #     self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "    # ):\n",
    "    #     \"\"\"Detects objects in a single image\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     y_pred, weights: array-like\n",
    "    #         Output from model\n",
    "    #     alpha, beta: float\n",
    "    #         Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "    #     cutoff: float\n",
    "    #         Threshold for detection\n",
    "    #     mode: string\n",
    "    #         Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "    #         `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "    #         score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "    #     \"\"\"\n",
    "    #     score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "    #     return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "    #     \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "    #     Parameters\n",
    "    #         ----------\n",
    "    #     pred, score: array-like\n",
    "    #         Output from model, score-map\n",
    "    #     cutoff, mode: float, string\n",
    "    #         Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "    #         `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "    #         score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "    #     \"\"\"\n",
    "    #     score = score[3:-3, 3:-3]\n",
    "    #     th = cutoff\n",
    "    #     if mode == \"quantile\":\n",
    "    #         th = np.quantile(score, cutoff)\n",
    "    #     elif mode == \"ratio\":\n",
    "    #         th = np.max(score.flatten()) * cutoff\n",
    "    #     hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "    #     hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "    #     detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "    #     return np.array(detections)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def local_consistency(pred):\n",
    "    #     \"\"\"Calculate the consistency metric\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     pred : array-like\n",
    "    #         first output from model\n",
    "    #     \"\"\"\n",
    "    #     pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "    #     kernel = np.ones((3, 3, 1)) / 3**2\n",
    "    #     pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "    #     squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "    #     squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "    #     np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "    #     return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "    # @classmethod\n",
    "    # def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "    #     \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     pred, weights: array-like\n",
    "    #         Output from model\n",
    "    #     alpha, beta: float\n",
    "    #         Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "    #     \"\"\"\n",
    "    #     return (\n",
    "    #         weights[0].detach().cpu().numpy() ** alpha\n",
    "    #         * cls.local_consistency(pred) ** beta\n",
    "    #     )\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        batch, class_label = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "        return (x, class_label), inverse\n",
    "\n",
    "    def val_preprocess(self, batch):\n",
    "        batch,_,_ = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        return (x,), inverse\n",
    "\n",
    "    test_preprocess = val_preprocess\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.eval()\n",
    "        return super().on_train_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import numpy as np\n",
    "\n",
    "lodestar = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_lodestar = dl.Trainer(max_epochs=150, accelerator='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_lodestar.fit(lodestar, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_size = 96\n",
    "test_optics = dt.Fluorescence(output_region=(0, 0, test_image_size, test_image_size))\n",
    "\n",
    "test_particle1 = dt.Ellipse(\n",
    "            intensity = 5.0,\n",
    "            position_unit=\"pixel\",\n",
    "            position=lambda: uniform(test_image_size / 2 - 30, test_image_size / 2 + 30, size=2),\n",
    "            radius=(5, 5)*dt.units.pixel,\n",
    "        )\n",
    "\n",
    "test_particle2 = dt.Ellipse(\n",
    "            intensity = 5.0,\n",
    "            position_unit=\"pixel\",\n",
    "            position=lambda: uniform(test_image_size / 2 - 30, test_image_size / 2 + 30, size=2),\n",
    "            radius=(5, 12)*dt.units.pixel,\n",
    "            rotation=lambda: np.random.uniform(0, 2 * np.pi),   \n",
    "        )\n",
    "\n",
    "\n",
    "simulation = (\n",
    "    test_optics(test_particle1 >> test_particle2)\n",
    "    >> dt.NormalizeMinMax(0, 1)\n",
    "    >> dt.Gaussian(sigma=0.1)\n",
    "    >> dt.MoveAxis(-1, 0)\n",
    "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = dt.pytorch.Dataset(simulation & test_particle1.position & test_particle2.position, length=100)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(5):\n",
    "\timage, position1, position2 = test_dataset[int(i+50)]\n",
    "\tplt.subplot(1, 5, i + 1)\n",
    "\tplt.imshow(image[0], cmap=\"gray\", origin=\"lower\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "# test_dataset = dt.pytorch.Dataset(simulation1 & particle1.position & particle2.position, length=100)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "lodestar.eval()\n",
    "data,pos1,pos2 = next(iter(test_dataloader))\n",
    "output = lodestar((data,pos1,pos2)).detach()\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(4):\n",
    "\tplt.subplot(2, 4, i + 1)\n",
    "\tif i==0:\n",
    "\t\tplt.imshow(data[1,...].squeeze(), cmap=\"gray\", origin=\"lower\")\n",
    "\t\tplt.title('input')\n",
    "\telse:\n",
    "\t\tplt.imshow(output[1,3+i,...], cmap=\"gray\", origin=\"lower\", vmin=0,vmax=1)\n",
    "\t\tplt.title('mask ' + str(i-1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.imshow(output[1,1,...].squeeze(), cmap=\"gray\", origin=\"lower\", vmin=0,vmax=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1,1,...].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplay_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
